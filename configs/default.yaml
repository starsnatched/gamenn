model:
  pretrained: google/gemma-3-270m-it
  quantization_4bit: true
  lora_r: 8
  lora_alpha: 16
  lora_dropout: 0.05
  target_modules: [q_proj, k_proj, v_proj, o_proj]
  max_length: 32768
  temperature: 0.7
  top_k: 32
  use_gradient_checkpointing: true
mcts:
  num_sims: 64
  c_puct: 1.2
  dirichlet_alpha: 0.3
  dirichlet_frac: 0.25
  progressive_widening_k: 1.0
  progressive_widening_alpha: 0.5
  root_top_k: 64
curriculum:
  enabled: true
  window: 64
  increase_threshold: 0.7
  decrease_threshold: 0.4
  max_sims: 256
  min_sims: 16
training:
  batch_size: 1
  mini_batch_size: 1
  ppo_epochs: 1
  learning_rate: 5e-5
  kl_target: 0.05
  seed: 42
  steps: 10
  rollout_steps: 1
dataset:
  path: null
  field_prompt: prompt
  max_samples: 8
logging:
  level: INFO
  out_dir: runs

